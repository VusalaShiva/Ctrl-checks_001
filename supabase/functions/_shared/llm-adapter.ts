// LLM Adapter Layer for CtrlChecks AI
// Unified interface for all LLM providers (OpenAI, Claude, Gemini)
// Future: Support for local inference models

export interface LLMMessage {
  role: 'system' | 'user' | 'assistant';
  content: string;
}

export interface LLMOptions {
  model: string;
  temperature?: number;
  maxTokens?: number;
  apiKey?: string;
  stream?: boolean;
}

export interface LLMResponse {
  content: string;
  usage?: {
    promptTokens: number;
    completionTokens: number;
    totalTokens: number;
  };
  model?: string;
  finishReason?: string;
}

export interface EmbeddingResponse {
  embedding: number[];
  model?: string;
}

export type LLMProvider = 'openai' | 'claude' | 'gemini';

/**
 * Unified LLM Adapter
 * Provides consistent interface across all LLM providers
 */
export class LLMAdapter {
  /**
   * Chat completion using any provider
   */
  async chat(
    provider: LLMProvider,
    messages: LLMMessage[],
    options: LLMOptions
  ): Promise<LLMResponse> {
    switch (provider) {
      case 'openai':
        return this.chatOpenAI(messages, options);
      case 'claude':
        return this.chatClaude(messages, options);
      case 'gemini':
        return this.chatGemini(messages, options);
      default:
        throw new Error(`Unsupported provider: ${provider}`);
    }
  }

  /**
   * Generate embeddings
   */
  async embed(
    provider: 'openai' | 'gemini',
    text: string,
    apiKey?: string
  ): Promise<EmbeddingResponse> {
    switch (provider) {
      case 'openai':
        return this.embedOpenAI(text, apiKey);
      case 'gemini':
        return this.embedGemini(text, apiKey);
      default:
        throw new Error(`Embedding not supported for provider: ${provider}`);
    }
  }

  /**
   * OpenAI Chat Completion
   */
  private async chatOpenAI(
    messages: LLMMessage[],
    options: LLMOptions
  ): Promise<LLMResponse> {
    const apiKey = options.apiKey || Deno.env.get('OPENAI_API_KEY');
    if (!apiKey) {
      throw new Error('OpenAI API key required. Provide apiKey in options or set OPENAI_API_KEY environment variable.');
    }

    // Map model names to OpenAI format
    const modelMap: Record<string, string> = {
      'gpt-4o': 'gpt-4o',
      'gpt-4o-mini': 'gpt-4o-mini',
      'gpt-4-turbo': 'gpt-4-turbo',
      'gpt-4': 'gpt-4',
      'gpt-3.5-turbo': 'gpt-3.5-turbo',
    };

    const model = modelMap[options.model] || options.model || 'gpt-4o';

    try {
      const response = await fetch('https://api.openai.com/v1/chat/completions', {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${apiKey}`,
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          model,
          messages: messages.map(msg => ({
            role: msg.role,
            content: msg.content,
          })),
          temperature: options.temperature ?? 0.7,
          max_tokens: options.maxTokens,
          stream: options.stream || false,
        }),
      });

      if (!response.ok) {
        const errorText = await response.text();
        let errorMessage = `OpenAI API error: ${response.status}`;
        
        try {
          const errorJson = JSON.parse(errorText);
          errorMessage += ` - ${errorJson.error?.message || errorText}`;
        } catch {
          errorMessage += ` - ${errorText}`;
        }
        
        throw new Error(errorMessage);
      }

      const data = await response.json();
      
      return {
        content: data.choices[0]?.message?.content || '',
        usage: data.usage ? {
          promptTokens: data.usage.prompt_tokens || 0,
          completionTokens: data.usage.completion_tokens || 0,
          totalTokens: data.usage.total_tokens || 0,
        } : undefined,
        model: data.model,
        finishReason: data.choices[0]?.finish_reason,
      };
    } catch (error) {
      if (error instanceof Error) {
        throw error;
      }
      throw new Error(`OpenAI API request failed: ${String(error)}`);
    }
  }

  /**
   * Anthropic Claude Chat Completion
   */
  private async chatClaude(
    messages: LLMMessage[],
    options: LLMOptions
  ): Promise<LLMResponse> {
    const apiKey = options.apiKey || Deno.env.get('ANTHROPIC_API_KEY');
    if (!apiKey) {
      throw new Error('Anthropic API key required. Provide apiKey in options or set ANTHROPIC_API_KEY environment variable.');
    }

    // Map model names to Claude format
    const modelMap: Record<string, string> = {
      'claude-3-5-sonnet': 'claude-3-5-sonnet-20241022',
      'claude-3-5-haiku': 'claude-3-5-haiku-20241022',
      'claude-3-opus': 'claude-3-opus-20240229',
      'claude-3-sonnet': 'claude-3-sonnet-20240229',
      'claude-3-haiku': 'claude-3-haiku-20240307',
    };

    const model = modelMap[options.model] || options.model || 'claude-3-5-sonnet-20241022';

    // Convert messages to Claude format
    // Claude uses a different message structure
    const systemMessage = messages.find(m => m.role === 'system');
    const conversationMessages = messages
      .filter(m => m.role !== 'system')
      .map(msg => ({
        role: msg.role === 'assistant' ? 'assistant' : 'user',
        content: msg.content,
      }));

    try {
      const response = await fetch('https://api.anthropic.com/v1/messages', {
        method: 'POST',
        headers: {
          'x-api-key': apiKey,
          'anthropic-version': '2023-06-01',
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          model,
          max_tokens: options.maxTokens || 4096,
          temperature: options.temperature ?? 0.7,
          system: systemMessage?.content,
          messages: conversationMessages,
        }),
      });

      if (!response.ok) {
        const errorText = await response.text();
        let errorMessage = `Claude API error: ${response.status}`;
        
        try {
          const errorJson = JSON.parse(errorText);
          errorMessage += ` - ${errorJson.error?.message || errorText}`;
        } catch {
          errorMessage += ` - ${errorText}`;
        }
        
        throw new Error(errorMessage);
      }

      const data = await response.json();
      
      // Claude returns content as an array
      const content = data.content
        .map((block: any) => block.text)
        .join('');

      return {
        content,
        usage: data.usage ? {
          promptTokens: data.usage.input_tokens || 0,
          completionTokens: data.usage.output_tokens || 0,
          totalTokens: (data.usage.input_tokens || 0) + (data.usage.output_tokens || 0),
        } : undefined,
        model: data.model,
        finishReason: data.stop_reason,
      };
    } catch (error) {
      if (error instanceof Error) {
        throw error;
      }
      throw new Error(`Claude API request failed: ${String(error)}`);
    }
  }

  /**
   * Google Gemini Chat Completion
   */
  private async chatGemini(
    messages: LLMMessage[],
    options: LLMOptions
  ): Promise<LLMResponse> {
    const apiKey = options.apiKey || Deno.env.get('GEMINI_API_KEY');
    if (!apiKey) {
      throw new Error('Gemini API key required. Provide apiKey in options or set GEMINI_API_KEY environment variable.');
    }

    // Map model names to Gemini format
    const modelMap: Record<string, string> = {
      'gemini-2.5-flash': 'gemini-2.5-flash',
      'gemini-2.5-pro': 'gemini-2.5-pro',
      'gemini-2.5-flash-lite': 'gemini-2.5-flash-lite',
      'gemini-pro': 'gemini-pro',
      'gemini-1.5-pro': 'gemini-1.5-pro',
    };

    const model = modelMap[options.model] || options.model || 'gemini-2.5-flash';

    // Convert messages to Gemini format
    // Gemini uses a different structure with parts
    const systemInstruction = messages.find(m => m.role === 'system')?.content;
    const conversationParts = messages
      .filter(m => m.role !== 'system')
      .map(msg => ({
        role: msg.role === 'assistant' ? 'model' : 'user',
        parts: [{ text: msg.content }],
      }));

    try {
      const url = `https://generativelanguage.googleapis.com/v1beta/models/${model}:generateContent?key=${apiKey}`;
      
      const response = await fetch(url, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          contents: conversationParts,
          systemInstruction: systemInstruction ? {
            parts: [{ text: systemInstruction }],
          } : undefined,
          generationConfig: {
            temperature: options.temperature ?? 0.7,
            maxOutputTokens: options.maxTokens,
          },
        }),
      });

      if (!response.ok) {
        const errorText = await response.text();
        let errorMessage = `Gemini API error: ${response.status}`;
        
        try {
          const errorJson = JSON.parse(errorText);
          errorMessage += ` - ${errorJson.error?.message || errorText}`;
        } catch {
          errorMessage += ` - ${errorText}`;
        }
        
        throw new Error(errorMessage);
      }

      const data = await response.json();
      
      const content = data.candidates?.[0]?.content?.parts?.[0]?.text || '';
      const usageInfo = data.usageMetadata;

      return {
        content,
        usage: usageInfo ? {
          promptTokens: usageInfo.promptTokenCount || 0,
          completionTokens: usageInfo.candidatesTokenCount || 0,
          totalTokens: usageInfo.totalTokenCount || 0,
        } : undefined,
        model: data.model || model,
        finishReason: data.candidates?.[0]?.finishReason,
      };
    } catch (error) {
      if (error instanceof Error) {
        throw error;
      }
      throw new Error(`Gemini API request failed: ${String(error)}`);
    }
  }

  /**
   * OpenAI Embeddings
   */
  private async embedOpenAI(
    text: string,
    apiKey?: string
  ): Promise<EmbeddingResponse> {
    const key = apiKey || Deno.env.get('OPENAI_API_KEY');
    if (!key) {
      throw new Error('OpenAI API key required for embeddings');
    }

    try {
      const response = await fetch('https://api.openai.com/v1/embeddings', {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${key}`,
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          model: 'text-embedding-3-small', // 1536 dimensions
          input: text,
        }),
      });

      if (!response.ok) {
        const errorText = await response.text();
        throw new Error(`OpenAI Embeddings API error: ${response.status} - ${errorText}`);
      }

      const data = await response.json();
      return {
        embedding: data.data[0].embedding,
        model: data.model,
      };
    } catch (error) {
      if (error instanceof Error) {
        throw error;
      }
      throw new Error(`OpenAI Embeddings request failed: ${String(error)}`);
    }
  }

  /**
   * Gemini Embeddings (if supported)
   */
  private async embedGemini(
    text: string,
    apiKey?: string
  ): Promise<EmbeddingResponse> {
    // Note: Gemini embeddings may require different endpoint
    // This is a placeholder - implement based on Gemini's embedding API
    throw new Error('Gemini embeddings not yet implemented. Use OpenAI for embeddings.');
  }

  /**
   * Detect provider from model name
   */
  static detectProvider(model: string): LLMProvider {
    if (model.startsWith('gpt-') || model.includes('openai')) {
      return 'openai';
    }
    if (model.startsWith('claude-') || model.includes('anthropic')) {
      return 'claude';
    }
    if (model.startsWith('gemini-') || model.includes('gemini')) {
      return 'gemini';
    }
    // Default to OpenAI
    return 'openai';
  }

  /**
   * Get available models for a provider
   */
  static getAvailableModels(provider: LLMProvider): string[] {
    switch (provider) {
      case 'openai':
        return [
          'gpt-4o',
          'gpt-4o-mini',
          'gpt-4-turbo',
          'gpt-4',
          'gpt-3.5-turbo',
        ];
      case 'claude':
        return [
          'claude-3-5-sonnet',
          'claude-3-5-haiku',
          'claude-3-opus',
          'claude-3-sonnet',
          'claude-3-haiku',
        ];
      case 'gemini':
        return [
          'gemini-2.5-flash',
          'gemini-2.5-pro',
          'gemini-2.5-flash-lite',
          'gemini-pro',
          'gemini-1.5-pro',
        ];
      default:
        return [];
    }
  }
}

